{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx_d7GMMf8zU"
   },
   "source": [
    "#Logging into git and cloning it into the colab runtime files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12hkuFovey59",
    "outputId": "53e953e7-9952-4434-983e-9112c309cc1f"
   },
   "outputs": [],
   "source": [
    "#!git version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYPSPqiMe3Fp"
   },
   "outputs": [],
   "source": [
    "#!git config --global user.email “hiscott2000@gmail.com”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aVJ0Z8DfWtn"
   },
   "outputs": [],
   "source": [
    "#!git config --global user.name “ScottGrant528”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-RVOPtxfjLw",
    "outputId": "01de6d94-4eff-4cc1-ea8a-5c828043b569"
   },
   "outputs": [],
   "source": [
    "#!git clone https://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@github.com/ScottGrant528/honours.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdhMIwRzlcSG"
   },
   "source": [
    "#Car Price Prediction Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euVdpwa8Ir9b"
   },
   "source": [
    "**Things to do:**\n",
    "\n",
    "- [X] Plot price range and compare to a gassian curver - useful for report\n",
    "- [X] Change tripplet generator to use price range instead of make and model\n",
    "- [ ] Implement a simple MLP\n",
    "- [ ] Crossfold - evaluation emethod\n",
    "tests how robust the function is\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "- [ ]  Plot loss curve\n",
    "\n",
    "Start with a simple mlp, can make it more elabrite as we go. Embedding layer to calc distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-ArgezMEggiU"
   },
   "outputs": [
    {
     "ename": "AlreadyExistsError",
     "evalue": "Another metric with the same name already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, metrics, Model\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/__init__.py:469\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_current_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    468\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[43m_keras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/distribute/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras' Distribution Strategy library.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sidecar_evaluator\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/distribute/sidecar_evaluator.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecation\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_experimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     optimizer \u001b[38;5;28;01mas\u001b[39;00m optimizer_experimental,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m     27\u001b[0m _PRINT_EVAL_STEP_EVERY_SEC \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60.0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Imports needed for deserialization.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adadelta \u001b[38;5;28;01mas\u001b[39;00m adadelta_legacy\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adagrad \u001b[38;5;28;01mas\u001b[39;00m adagrad_legacy\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adam \u001b[38;5;28;01mas\u001b[39;00m adam_legacy\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers/legacy/adadelta.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Legacy Adadelta optimizer implementation.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adadelta\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adadelta.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_config\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v2\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m---> 38\u001b[0m keras_optimizers_gauge \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonitoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolGauge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tensorflow/api/keras/optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras optimizer usage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m _DEFAULT_VALID_DTYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(\n\u001b[1;32m     43\u001b[0m     [\n\u001b[1;32m     44\u001b[0m         tf\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     ]\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deduplicate_indexed_slices\u001b[39m(values, indices):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py:356\u001b[0m, in \u001b[0;36mBoolGauge.__init__\u001b[0;34m(self, name, description, *labels)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, description, \u001b[38;5;241m*\u001b[39mlabels):\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new BoolGauge.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    *labels: The label list of the new metric.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBoolGauge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBoolGauge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_bool_gauge_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/monitoring.py:131\u001b[0m, in \u001b[0;36mMetric.__init__\u001b[0;34m(self, metric_name, metric_methods, label_length, *args)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods):\n\u001b[1;32m    128\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot create \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m metric with label >= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    129\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_name, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods)))\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_methods\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_label_length\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Another metric with the same name already exists."
     ]
    }
   ],
   "source": [
    "#global imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, metrics, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZdHMYmzlwVa"
   },
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L4mFZN89lUYU"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datasets/UK_Used_Car_Set/audi.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audi \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/datasets/UK_Used_Car_Set/audi.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m bmw \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/datasets/UK_Used_Car_Set/bmw.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m ford \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/datasets/UK_Used_Car_Set/ford.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/UK_Used_Car_Set/audi.csv'"
     ]
    }
   ],
   "source": [
    "audi = pd.read_csv('/datasets/UK_Used_Car_Set/audi.csv')\n",
    "bmw = pd.read_csv('/datasets/UK_Used_Car_Set/bmw.csv')\n",
    "ford = pd.read_csv('/datasets/UK_Used_Car_Set/ford.csv')\n",
    "hyundi = pd.read_csv('/datasets/UK_Used_Car_Set/hyundi.csv')\n",
    "merc = pd.read_csv('/datasets/UK_Used_Car_Set/merc.csv')\n",
    "skoda = pd.read_csv('/datasets/UK_Used_Car_Set/skoda.csv')\n",
    "toyota = pd.read_csv('/datasets/UK_Used_Car_Set/toyota.csv')\n",
    "vauxhall = pd.read_csv('/datasets/UK_Used_Car_Set/vauxhall.csv')\n",
    "vw = pd.read_csv('/datasets/UK_Used_Car_Set/vw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXomJojLmWln"
   },
   "source": [
    "### Exploring the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XixEUXt3mV0q",
    "outputId": "e2f772c2-091e-47a5-d087-e51e5498540c"
   },
   "outputs": [],
   "source": [
    "#Storing the lengths\n",
    "audi_length = len(audi)\n",
    "bmw_length = len(bmw)\n",
    "\n",
    "ford_length = len(ford)\n",
    "hyundi_length = len(hyundi)\n",
    "merc_length = len(merc)\n",
    "skoda_length = len(skoda)\n",
    "toyota_length = len(toyota)\n",
    "vauxhall_length = len(vauxhall)\n",
    "vw_length = len(vw)\n",
    "\n",
    "# Print the lengths\n",
    "print(\"Length of audi:\", audi_length)\n",
    "print(\"Length of bmw:\", bmw_length)\n",
    "print(\"Length of ford:\", ford_length)\n",
    "print(\"Length of hyundi:\", hyundi_length)\n",
    "print(\"Length of merc:\", merc_length)\n",
    "print(\"Length of skoda:\", skoda_length)\n",
    "print(\"Length of toyota:\", toyota_length)\n",
    "print(\"Length of vauxhall:\", vauxhall_length)\n",
    "print(\"Length of vw:\", vw_length)\n",
    "\n",
    "# Calculate and print the total length\n",
    "total_length = sum([audi_length, bmw_length, ford_length, hyundi_length, merc_length, skoda_length, toyota_length, vauxhall_length, vw_length])\n",
    "print(\"\\nTotal size of the Dataset:\", total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XReCpjROma3U",
    "outputId": "a67ee0da-f1f7-4878-ac7b-66badb118beb"
   },
   "outputs": [],
   "source": [
    "#getting data types information\n",
    "audi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byo0xoFAmj-J",
    "outputId": "a0ffb3d3-1ba9-4553-8330-5dd84d7c4a8e"
   },
   "outputs": [],
   "source": [
    "#Exploring the unique values in the dataset\n",
    "\n",
    "### Edited code from Generative AI ###\n",
    "print(f'The unique values in the Audi dataset are:\\n')\n",
    "def get_unique_values(dataset):\n",
    "    unique_values_dict = {}\n",
    "    for column in dataset.columns:\n",
    "        unique_values = dataset[column].unique()\n",
    "        unique_values_dict[column] = unique_values\n",
    "    return unique_values_dict\n",
    "\n",
    "unique_values_result = get_unique_values(audi)\n",
    "for column, unique_values in unique_values_result.items():\n",
    "    print(f\"Unique values in the '{column}' column:\")\n",
    "    print(unique_values)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcGJ-Ib_nQh_",
    "outputId": "9b8fe777-b1a8-491a-ccb9-fac2d7388522"
   },
   "outputs": [],
   "source": [
    "#Checking unique values of the engine size column in closer detail\n",
    "engine_size_counts = audi['engineSize'].value_counts()\n",
    "print(engine_size_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiYn8PmmnhWu"
   },
   "source": [
    "The Car's with an engine size of '0.0' are electric cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oW1iwR54oj_v",
    "outputId": "15db6e3c-c040-4fd4-86da-3131db0255ea"
   },
   "outputs": [],
   "source": [
    "datasets = [audi, bmw, ford, hyundi, merc, skoda, toyota, vauxhall, vw]\n",
    "engine_size_zero = 0.0\n",
    "count = 0\n",
    "\n",
    "for df in datasets:\n",
    "    engine_size_counts = df['engineSize'].value_counts()\n",
    "    if 0.0 in engine_size_counts.index:\n",
    "           count += engine_size_counts[0.0]\n",
    "\n",
    "# Print the total number of engine sizes equal to 0.0 across all datasets\n",
    "print(f\"Total number of engine sizes equal to {engine_size_zero} across all datasets: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjJigu-Zrh3D"
   },
   "source": [
    "### Dataset Combination\n",
    "Combinding the datasets into a singular one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "KX1KKQ1KJSBV",
    "outputId": "48f3b92e-a502-45d8-9373-182a59aa0f87"
   },
   "outputs": [],
   "source": [
    "# Add a 'make' column to each DataFrame\n",
    "audi['make'] = 'audi'\n",
    "bmw['make'] = 'bmw'\n",
    "ford['make'] = 'ford'\n",
    "hyundi['make'] = 'hyundi'\n",
    "merc['make'] = 'mercedes'\n",
    "skoda['make'] = 'skoda'\n",
    "toyota['make'] = 'toyota'\n",
    "vauxhall['make'] = 'vauxhall'\n",
    "vw['make'] = 'volkswagen'\n",
    "hyundi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8WHoDnzaJrT0",
    "outputId": "4e708bfb-30f6-4876-e3c2-d51dd151ff06"
   },
   "outputs": [],
   "source": [
    "#Rename the column 'tax(£)' to 'tax'\n",
    "hyundi.rename(columns={'tax(£)': 'tax'}, inplace=True)\n",
    "hyundi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLi4l4-Orm6V"
   },
   "outputs": [],
   "source": [
    "#Making a combined used car dataset\n",
    "UsedCarDataset = pd.DataFrame()\n",
    "\n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "UsedCarDataset = pd.concat(datasets, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYJfRXFNjDOo",
    "outputId": "cfc73147-8166-469f-f089-d405f4125d29"
   },
   "outputs": [],
   "source": [
    "#Saving the initial size of the dataset\n",
    "BeforeSize = UsedCarDataset.shape[0]\n",
    "print(BeforeSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "IrYmTXGII7l5",
    "outputId": "5096942a-061c-4546-b74d-ff26a36151c7"
   },
   "outputs": [],
   "source": [
    "#Getting an idea of the dataset by looking at random sample data\n",
    "UsedCarDataset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG4vMZf2Ey5a"
   },
   "source": [
    "### Data Visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "58hcA570Fek8",
    "outputId": "d82d030d-df0c-445f-bd55-daf659520085"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgu91-JmpICS"
   },
   "outputs": [],
   "source": [
    "#Function to draw a bar plot for a categorical column\n",
    "def Barplot(column):\n",
    "  valueCount = UsedCarDataset[column].value_counts()\n",
    "  valueCount.plot(kind='bar')\n",
    "  plt.title(f'Distribution of \"{column}\"')\n",
    "  plt.xlabel(column)\n",
    "  plt.ylabel('Count')\n",
    "  plt.xticks(rotation=45)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BUhfhgu9gjWz",
    "outputId": "3dbaa5e3-72f7-4fe9-a211-e881f14cc5b9"
   },
   "outputs": [],
   "source": [
    "#Plotting fuelType, Transmission Type and the make of the cars\n",
    "Barplot('fuelType')\n",
    "Barplot('transmission')\n",
    "Barplot('make')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLYtrHZgqBp3"
   },
   "outputs": [],
   "source": [
    "#Scatter Plot for regression columns\n",
    "def ScatterPlot(column):\n",
    "  valueCount = UsedCarDataset[column].value_counts()\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.scatter(valueCount.index, valueCount.values, alpha=0.5)\n",
    "  plt.title(f'Car \"{column}\" vs. Count')\n",
    "  plt.xlabel(column)\n",
    "  plt.ylabel('Count')\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7AcSuQyBqD5o",
    "outputId": "3dcdaa45-bb6d-42d0-acff-c07253e73a54"
   },
   "outputs": [],
   "source": [
    "#Plotting car price and mileage\n",
    "ScatterPlot('price')\n",
    "ScatterPlot('mileage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZIdYDKBufnq"
   },
   "outputs": [],
   "source": [
    "def findAmountOver(column, number):\n",
    "  selectedCars = UsedCarDataset[UsedCarDataset[column] > number]\n",
    "  print(f'The number of cars with a {column} over {number} is: {selectedCars.shape[0]}')\n",
    "\n",
    "def findAmountUnder(column, number):\n",
    "  selectedCars = UsedCarDataset[UsedCarDataset[column] < number]\n",
    "  print(f'The number of cars with a {column} under {number} is: {selectedCars.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6veec7yyAg5",
    "outputId": "4ed27f18-fa1d-46f2-9db8-d67ab5bc452d"
   },
   "outputs": [],
   "source": [
    "#Finding the outliers\n",
    "#Mileage\n",
    "findAmountOver(\"mileage\", 110000)\n",
    "findAmountUnder(\"mileage\", 2)\n",
    "\n",
    "#Price\n",
    "findAmountOver(\"price\", 60000)\n",
    "findAmountUnder(\"price\", 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GutIRNwYyVlT"
   },
   "source": [
    "Cars **over 100,000 miles** will be **removed** from the output as there is too few of them. There is **no lower limit for mileage** as this dataset includes plenty cars with as little as 1 mileage.\n",
    "\n",
    " Cars **under £2,000** and **over £60,000** will be **removed** from the dataset as there is also too few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzUldasSKcOY"
   },
   "source": [
    "## Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqhAq17jV9UE"
   },
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6yVE66Ogpxn"
   },
   "source": [
    "Removing Electric cars. These have an engine size of 0 or a fuel type of electric, 'Other' fuel type will also be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCphOznHkfFo"
   },
   "outputs": [],
   "source": [
    "#Functions to drop a group of cars given a column and specific condition (i.e. fuelType = electric or price > 60,000)\n",
    "#Drops cars matching a specific condition\n",
    "def DropCars(column, specific):\n",
    "  df = UsedCarDataset[UsedCarDataset[column] == specific]\n",
    "  UsedCarDataset.drop(df.index, inplace=True)\n",
    "\n",
    "#Drops cars over a certain limit\n",
    "def DropCarsOver(column, limit):\n",
    "  df = UsedCarDataset[UsedCarDataset[column] > limit]\n",
    "  UsedCarDataset.drop(df.index, inplace=True)\n",
    "\n",
    "#Drops cars under a certain limit\n",
    "def DropCarsUnder(column, limit):\n",
    "  df = UsedCarDataset[UsedCarDataset[column] < limit]\n",
    "  UsedCarDataset.drop(df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vze_6G-GlBU4",
    "outputId": "d269655f-5714-4f70-c72d-077202f2a41a"
   },
   "outputs": [],
   "source": [
    "#Dropping cars where the engine size is 0\n",
    "DropCars('engineSize', 0)\n",
    "#Checking it was successful\n",
    "UsedCarDataset['engineSize'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKLISf4Zlf3F",
    "outputId": "fab13170-69ee-4bc4-eebb-99ef20ebc488"
   },
   "outputs": [],
   "source": [
    "#Dropping cars with a fuelType of other or electric\n",
    "DropCars('fuelType', 'Other')\n",
    "DropCars('fuelType', 'Electric')\n",
    "\n",
    "#Checks it was successful\n",
    "UsedCarDataset['fuelType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT8cjFJxrzBF"
   },
   "source": [
    "Removing the all engine sizes with under 100 enteries as these are too few\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjQgNIE_mOmP",
    "outputId": "1a70fa74-a710-47cc-d23f-6a2fdc9fbbd4"
   },
   "outputs": [],
   "source": [
    "# An Array of all engine sizes below 100\n",
    "engine_sizes_to_remove = [4.4,2.4,5.0,5.5,2.8,3.2,1.9,4.2,4.7,5.2,3.5,6.2,2.7,0.6,6.6,4.1,6.0,3.7,6.3,5.4,4.3,4.5]\n",
    "\n",
    "#Removes the cars listed in above array\n",
    "for x in engine_sizes_to_remove:\n",
    "  DropCars('engineSize', x)\n",
    "\n",
    "#Checks to see if removal was successful\n",
    "UsedCarDataset['engineSize'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PugranDIKAEF"
   },
   "source": [
    "Removing cars with transmission type of 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGa9OmoEIgFK",
    "outputId": "0d1bbb63-12cb-40bf-8239-713d6f537e2c"
   },
   "outputs": [],
   "source": [
    "DropCars('transmission','Other')\n",
    "UsedCarDataset['transmission'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOtCLO1Y2vEw"
   },
   "source": [
    "Removing Pricing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4_JZ51m2xha",
    "outputId": "6c871823-4a4c-4e3a-ddc0-2723a22aff8e"
   },
   "outputs": [],
   "source": [
    "#Dropping cars over £60000\n",
    "print('Before:')\n",
    "findAmountOver(\"price\", 60000)\n",
    "DropCarsOver('price', 60000)\n",
    "#Testing the cars were successfully removed\n",
    "print('After:')\n",
    "findAmountOver(\"price\", 60000)\n",
    "\n",
    "#Dropping cars under £2000\n",
    "print('Before:')\n",
    "findAmountUnder(\"price\", 2000)\n",
    "DropCarsUnder('price', 2000)\n",
    "print('After:')\n",
    "findAmountUnder(\"price\", 2000)\n",
    "\n",
    "#Dropping cars over 100,000 miles\n",
    "print('Before:')\n",
    "findAmountOver(\"mileage\", 100000)\n",
    "DropCarsOver('mileage', 100000)\n",
    "print('After:')\n",
    "findAmountOver(\"mileage\", 100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZXcGtqyd6FX"
   },
   "source": [
    "Removing Car Model Outliers with less than 10 cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOLo9GH_d8WU"
   },
   "outputs": [],
   "source": [
    "modelCount = UsedCarDataset['model'].value_counts()\n",
    "lowModelCount = modelCount[modelCount < 10].index\n",
    "#lowModelCount.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJyEZbUGehDj",
    "outputId": "49692b4a-8880-453e-8632-964926f4c352"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset = UsedCarDataset[~UsedCarDataset['model'].isin(lowModelCount)]\n",
    "UsedCarDataset['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7tH86WiHx_zu",
    "outputId": "af133640-435b-4814-af7e-8e6fffac3943"
   },
   "outputs": [],
   "source": [
    "print(f'Used Car Dataset size before pre-processing:{BeforeSize}')\n",
    "print(f'Used Car Dataset size after pre-processing:{UsedCarDataset.size}')\n",
    "print(f'There were {BeforeSize - UsedCarDataset.size} cars removed in pre-processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVmy0KhVMR2G"
   },
   "source": [
    "### Creating a price-range field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbUXPhe3nxUP"
   },
   "source": [
    "To covert this task of price estimation from a regression based approach to categorical, the price field will be rounded to the nearest 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMtdE1f3S7lr"
   },
   "outputs": [],
   "source": [
    "#function to round the price to the nearest interval\n",
    "### Adapted Generative AI Code ###\n",
    "def roundToNearest(price, interval):\n",
    "  rounded = round(price / interval) * interval\n",
    "  return rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcOW5B73T-go",
    "outputId": "aa00f26c-747b-474e-f137-398909254911"
   },
   "outputs": [],
   "source": [
    "#Testing Rounding function\n",
    "print(roundToNearest(1749.99, 500))\n",
    "print(roundToNearest(2450, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ql6mNvN6Ywh0"
   },
   "source": [
    "**Inserting the Price Range field**\n",
    "<br>This is a float64 type and is later converted to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Ob0hOMSbYstY",
    "outputId": "d7f5da8a-d8af-4f34-dbd4-64d92473f742"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset['price_range'] = roundToNearest(UsedCarDataset['price'], 500)\n",
    "UsedCarDataset.drop(columns=['price'], inplace=True)\n",
    "UsedCarDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rn96EqHxPdZ"
   },
   "source": [
    "### Visualising Price Range Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "eHVzlj_5hmxc",
    "outputId": "70c2bb8e-0392-44bb-bd6c-293f6a5ad411"
   },
   "outputs": [],
   "source": [
    "# Set the style of the plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot the distribution of price_range\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(UsedCarDataset['price_range'], bins=20, kde=False, color='blue', label='Price Range')\n",
    "\n",
    "# Overlay a Gaussian distribution\n",
    "mu, std = norm.fit(UsedCarDataset['price_range'])\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std) * len(UsedCarDataset['price_range']) * (xmax - xmin) / 20  # Scaling for comparison\n",
    "plt.plot(x, p, 'k', linewidth=2, label='Gaussian Fit')\n",
    "\n",
    "plt.title('Distribution of Price Ranges with Gaussian Fit')\n",
    "plt.xlabel('Price Range')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTWJYGom2B6U"
   },
   "source": [
    "### Turning string based features into numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8DZQGcIkoVy"
   },
   "source": [
    "**Changing transmission values to numericals**<br>\n",
    "Manual = 0 <br>\n",
    "Automatic = 1 <br>\n",
    "Semi-Auto = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvT0I94K2JJ1",
    "outputId": "b3b64f36-af18-4006-85f4-d6bf58c57d5b"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset['transmission'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gML951RjYMf"
   },
   "outputs": [],
   "source": [
    "def numericalFeatureTransformation(mapping_array, column):\n",
    "  UsedCarDataset[column] = UsedCarDataset[column].replace(mapping_array)\n",
    "  return UsedCarDataset[column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nH8WqDO4jzGJ",
    "outputId": "8fd4ebc3-5af2-4b34-c3cf-e593a9a03cd5"
   },
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "transmission_mapping = {'Manual': 0, 'Automatic': 1, 'Semi-Auto': 2}\n",
    "\n",
    "numericalFeatureTransformation(transmission_mapping, 'transmission')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3Qm1DhIk8Io"
   },
   "source": [
    "**Changing make values to numericals**<br>\n",
    "'ford':0<br>\n",
    "'volkswagen':1 <br>\n",
    "'vauxhall':2<br>\n",
    "'mercedes':3<br>\n",
    "'bmw':4 <br>\n",
    "'audi':5<br>\n",
    "'toyota':6<br>'skoda':7<br>\n",
    "'hyundi':8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yi2V6DONkT_b",
    "outputId": "f915fe18-6da0-46b4-ab08-8a648fa48760"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset['make'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9oB6D3RlYC4",
    "outputId": "3da6bb43-11dc-4587-897d-167bca993aca"
   },
   "outputs": [],
   "source": [
    "make_mapping = {'ford':0, 'volkswagen':1, 'vauxhall':2, 'mercedes':3, 'bmw':4, 'audi':5, 'toyota':6, 'skoda':7, 'hyundi':8}\n",
    "numericalFeatureTransformation(make_mapping, 'make')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpYDjeHJlrlq"
   },
   "source": [
    "**Changing Fuel Type to Numerical values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dVdzM23ylvR5",
    "outputId": "7d38cc44-b7cc-436d-951d-4ff5ef1e928c"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset['fuelType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWCw5ESxlsLr",
    "outputId": "fe173c77-ef9d-4560-a696-a14c650390c8"
   },
   "outputs": [],
   "source": [
    "fuel_mapping = {'Petrol':0, 'Diesel':1, 'Hybrid':2}\n",
    "numericalFeatureTransformation(fuel_mapping, 'fuelType')\n",
    "UsedCarDataset['fuelType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qw-aOpC7lQz6"
   },
   "source": [
    "**Changing model values to numberical values.** <br>\n",
    "This is done automatically as there is many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJzNg5_Kv2J2",
    "outputId": "b405f81e-6c1e-4413-964b-c45896c52105"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "o4cgqYHwcZTt",
    "outputId": "7414dc31-93a1-45bb-8a77-7bc7ff85f888"
   },
   "outputs": [],
   "source": [
    "### Adapted Generative AI Code ###\n",
    "\n",
    "#Counter to iterate through different models\n",
    "counter = 0\n",
    "\n",
    "#mapping dictonary to store mapping between model names and the number it is assosiated with\n",
    "model_mapping = {}\n",
    "\n",
    "for model in UsedCarDataset['model']:\n",
    "  #Check to see if model has been mapped already\n",
    "  if model not in model_mapping:\n",
    "    #if so then assign a new number to it\n",
    "    model_mapping[model] = counter\n",
    "    counter += 1\n",
    "\n",
    "#Now apply this to the dataset\n",
    "UsedCarDataset['model'] = UsedCarDataset['model'].map(model_mapping)\n",
    "UsedCarDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "iEuBYP63dhRy",
    "outputId": "e5f3a659-5dbe-4e14-e008-d1adc6e1e720"
   },
   "outputs": [],
   "source": [
    "# Convert the model mapping dictionary to a DataFrame\n",
    "model_map_df = pd.DataFrame(list(model_mapping.items()), columns=['Model', 'Numeric Identifier'])\n",
    "\n",
    "# Print the DataFrame\n",
    "model_map_df.head(151)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4NVxFLhmak0"
   },
   "source": [
    "Changing object types to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NB4jXm7rmKih",
    "outputId": "db30b5e9-70f1-4747-dafe-3d2096e0235f"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9IIcWQVmg7x"
   },
   "outputs": [],
   "source": [
    "#function to change a column into an integer\n",
    "def toInt(column):\n",
    "  UsedCarDataset[column] = UsedCarDataset[column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ns5-2wv4mrRS"
   },
   "outputs": [],
   "source": [
    "#Putting colums types to int\n",
    "toInt('model')\n",
    "toInt('transmission')\n",
    "toInt('price_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AGFi-Kjm6xg",
    "outputId": "2a5b4350-00fe-40f8-d7a3-ae9b77223682"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxnYjLeRrSWk"
   },
   "source": [
    "Changing year to age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nDsBONOQrWm8",
    "outputId": "2e996333-2cb2-46f2-830b-71ac49842358"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset['age'] = 2024 - UsedCarDataset['year']\n",
    "UsedCarDataset.drop(columns=['year'], inplace=True)\n",
    "UsedCarDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gC9_Gcp-r4qX",
    "outputId": "95fc63f6-314a-47cb-d869-a5e643a218c6"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7R388m5sQWg"
   },
   "source": [
    "From the value counts method we see there is a negative age, we need to drop this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utS6x2DdsK73",
    "outputId": "a80caebf-ec8d-4a5a-da41-7bf79a294830"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset = UsedCarDataset[UsedCarDataset['age'] >= 0]\n",
    "UsedCarDataset['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi7okd6muHoo"
   },
   "source": [
    "### Splitting the dataset into X and Y\n",
    "\n",
    "X is the features (Engine, transmission, mileage etc) <br>\n",
    "Y is the Target Variable (price)\n",
    "\n",
    "Code modifed from source: https://builtin.com/data-science/train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QVYmFr8njlUC",
    "outputId": "184198fe-da1c-4dc8-a344-dbda2554ebed"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJICVFyquV0I"
   },
   "outputs": [],
   "source": [
    "features = ['make', 'model', 'transmission', 'mileage', 'fuelType' , 'tax', 'mpg', 'engineSize', 'age']\n",
    "X_UsedCarDataset = UsedCarDataset.loc[:, features]\n",
    "y_UsedCarDataset = UsedCarDataset.loc[:, 'price_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rJDrbnk1vDws",
    "outputId": "cb62a595-db9d-43cc-e784-5d79dbab2ed0"
   },
   "outputs": [],
   "source": [
    "X_UsedCarDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "122BixdUk7PN",
    "outputId": "e05985d5-f720-4504-c3d4-0297e54678bc"
   },
   "outputs": [],
   "source": [
    "y_UsedCarDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDGS99d_xs_7"
   },
   "outputs": [],
   "source": [
    "#Converting Both into Numpy arrays\n",
    "#y_UsedCarDataset = y_UsedCarDataset.values\n",
    "#X_UsedCarDataset = X_UsedCarDataset.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMJfn3NXj-5z"
   },
   "source": [
    "### Pre-Processing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNC4C2GIkBuC",
    "outputId": "1a6ff897-08ad-42f8-f0c9-5318a8968946"
   },
   "outputs": [],
   "source": [
    "print(f'Used Car Dataset size before pre-processing:{BeforeSize}')\n",
    "print(f'Used Car Dataset size after pre-processing:{UsedCarDataset.shape[0]}')\n",
    "print(f'There were {BeforeSize - UsedCarDataset.shape[0]} cars removed in pre-processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UsUx5T37tltL",
    "outputId": "ccf57c89-1d1e-493a-cb3c-fa97ed84a8c6"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEeWIpXXZzeQ"
   },
   "source": [
    "## Creating the Triplet Network Deep Metric Learner\n",
    "\n",
    "Based on Keras's Siamese Network with a triplet loss available at: https://keras.io/examples/vision/siamese_network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YySU5xktBO-9"
   },
   "source": [
    "Things to do\n",
    "- Plot loss curve\n",
    "- Plot price range and compare to a gassian curver - useful for report\n",
    "- Crossfold - evaluation emethod\n",
    "tests how robust the function is\n",
    "stratified varient\n",
    "- Change tripplet generator to use price range instead of make and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxJuHgk1aHv5"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI4HIVLPqnF3"
   },
   "source": [
    "### Creating the Triplets (Positive, Anker and Negaive)\n",
    "\n",
    "Anker is the reference data point <br>\n",
    "Positive is a similar data point <br>\n",
    "Negative is a not similar data point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tlj3X6adpTp2"
   },
   "outputs": [],
   "source": [
    "#Reducing Dataset size due to extreme ram usage\n",
    "#Sampling a small amount of the dataset\n",
    "#num_samples_to_keep = int(0.2 * len(UsedCarDataset))\n",
    "#UsedCarDataset = UsedCarDataset.sample(n=num_samples_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5HsxuVdpkV3",
    "outputId": "ce9d559a-8e3f-4b7a-a042-2386d718a2c9"
   },
   "outputs": [],
   "source": [
    "UsedCarDataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ8Dt_bFnT_G"
   },
   "outputs": [],
   "source": [
    "#Spliting datasets into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_UsedCarDataset, y_UsedCarDataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSs1LvdkyZq_",
    "outputId": "502b3e6e-6bdb-4e91-c8f7-ea0ff50ba665"
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx45QV7YH098"
   },
   "source": [
    "Creates a set of triplets based on the price_range and make of the car.\n",
    "- The positive sample is from the same make and model\n",
    "- While the negative sample is from neither\n",
    "- A function could be created to use more factors in creating a better positive sample\n",
    "- Could test with and without selecting make - speak about in report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaLrXue_Ix8F"
   },
   "outputs": [],
   "source": [
    "def create_triplets(x, y):\n",
    "  \"\"\"\n",
    "  Create triplets from the datasets x and y.\n",
    "  Each triplet consists of an anchor, positive, and negative sample.\n",
    "  \"\"\"\n",
    "  #Initalising array to return\n",
    "  triplets = []\n",
    "\n",
    "  #Iterating through x\n",
    "  for index, row in x.iterrows():\n",
    "    #Extract car make and price_range into their own variables\n",
    "    make = row['make']\n",
    "    price_range = y.loc[index] #Access corrosponding index\n",
    "\n",
    "    #Selecting postive sample by selecting all data points that are the same make and in the same price_range\n",
    "    #A random sample is then taken of one of the data points\n",
    "    positive_sample = x[(y == price_range) & (x['make'] == make)].sample(n=1).squeeze().tolist()\n",
    "    #print(positive_sample)\n",
    "    #Same again but for a different make and differnt price range\n",
    "    negative_sample = x[(y != price_range) & (x['make'] != make)].sample(n=1).squeeze().tolist()\n",
    "\n",
    "    anchor = row.values.tolist()\n",
    "    triplets.append([anchor, positive_sample, negative_sample])\n",
    "  return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw-IeE99vgPT"
   },
   "source": [
    "Visualisation Function to display 3 triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjsDo4csyZSm"
   },
   "outputs": [],
   "source": [
    "def find_index(sample, X_UsedCarDataset):\n",
    "  \"\"\"\n",
    "  Find the index of the anchor sample in X_UsedCarDataset that matches the given anchor triplet.\n",
    "  \"\"\"\n",
    "  for index, row in X_UsedCarDataset.iterrows():\n",
    "    #check if row values match the anchor triplet\n",
    "    if row.tolist() == sample:\n",
    "      return index\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAnTBLuk8iOx"
   },
   "outputs": [],
   "source": [
    "def visualize_triplet(triplet, y_train):\n",
    "    \"\"\"\n",
    "    Visualize a random triplet from the dataset along with their price_range from y_train.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    random_number = np.random.randint(len(triplet))\n",
    "\n",
    "    # Anchor sample with price_range\n",
    "    anchor_with_price = f'{triplet[random_number][0]}\\nPrice Range: {y_train[find_index(triplet[random_number][0], X_UsedCarDataset)]}'\n",
    "    axs[0].set_title('Anchor Sample')\n",
    "    axs[0].text(0.5, 0.5, anchor_with_price, ha='center', va='center', fontsize=12, color='gray')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Positive sample with price_range\n",
    "    positive_with_price = f'{triplet[random_number][1]}\\nPrice Range: {y_train[find_index(triplet[random_number][1], X_UsedCarDataset)]}'\n",
    "    axs[1].set_title('Positive Sample')\n",
    "    axs[1].text(0.5, 0.5, positive_with_price, ha='center', va='center', fontsize=12, color='gray')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Negative sample with price_range\n",
    "    negative_with_price = f'{triplet[random_number][2]}\\nPrice Range: {y_train[find_index(triplet[random_number][2], X_UsedCarDataset)]}'\n",
    "    axs[2].set_title('Negative Sample')\n",
    "    axs[2].text(0.5, 0.5, negative_with_price, ha='center', va='center', fontsize=12, color='gray')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtu3CSWeB-Fw"
   },
   "source": [
    "Creating Triplets from the X and y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbjqK62rxoCL"
   },
   "outputs": [],
   "source": [
    "#Triplets from train sets\n",
    "train_triplets = create_triplets(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qfhy3Z38Mm2W",
    "outputId": "720cc364-5924-4b2e-e2a6-80fc8f67053d"
   },
   "outputs": [],
   "source": [
    "print(train_triplets[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy0vaK2ECPps"
   },
   "source": [
    "Visualising the Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "1_Gp-L502kda",
    "outputId": "7ee5647f-09ce-49e5-b4a0-e4eacdbf8f04"
   },
   "outputs": [],
   "source": [
    "# Visualize a random triplet\n",
    "print('Samples from Train Set\\n')\n",
    "visualize_triplet(train_triplets, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9FSRkxEljwB"
   },
   "source": [
    "### Creating the Multi-Layer Peceptron with Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INg7doUbsprS"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRCW4kRFyaiG"
   },
   "outputs": [],
   "source": [
    "#Fixes 'cannot import name 'ops' python'\n",
    "#!pip install tensorflow --upgrade\n",
    "#!pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "eT-B_AUpsXe1",
    "outputId": "1f62a23b-c4c7-4e79-b576-57ad167a123b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras.layers import Input, Flatten\n",
    "from keras import losses\n",
    "from keras import ops\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras import Model\n",
    "from keras.applications import resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFS18ztlsm03"
   },
   "source": [
    "Setting Up the embedding generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bR1fN7U3sqi5"
   },
   "outputs": [],
   "source": [
    "#Anchor, Positive, Negative each with 9 features\n",
    "target_shape = (9,)\n",
    "inputs = Input(shape=target_shape)\n",
    "\n",
    "flatten = layers.Flatten()(inputs)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(128)(dense2)\n",
    "embedding = Model(inputs, output, name='Embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXHlGsNRvFT5"
   },
   "source": [
    "Setting up the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ocu0fhXcI3TS"
   },
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = ops.sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = ops.sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=target_shape)\n",
    "positive_input = layers.Input(name=\"positive\", shape=target_shape)\n",
    "negative_input = layers.Input(name=\"negative\", shape=target_shape)\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(anchor_input),\n",
    "    embedding(positive_input),\n",
    "    embedding(negative_input),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qS33Ef9m3xSU"
   },
   "source": [
    "Building the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azFl8VEG3ucW"
   },
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rl5ZaIgxGp2t"
   },
   "outputs": [],
   "source": [
    "# model summary (see the number of trainable parameters)\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhpshW5b7_Sl"
   },
   "outputs": [],
   "source": [
    "anchors = []\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "for triplet in train_triplets:\n",
    "    anchors.append(triplet[0])\n",
    "    positives.append(triplet[1])\n",
    "    negatives.append(triplet[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLSzfMJP8gWw"
   },
   "outputs": [],
   "source": [
    "# Convert lists to TensorFlow datasets\n",
    "anchor_dataset = tf.data.Dataset.from_tensor_slices(anchors)\n",
    "positive_dataset = tf.data.Dataset.from_tensor_slices(positives)\n",
    "negative_dataset = tf.data.Dataset.from_tensor_slices(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enRnEmfl8Ays"
   },
   "outputs": [],
   "source": [
    "print(anchor_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGzUq6Qy11bA"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "\n",
    "train_dataset = dataset.take(round(len(dataset) * 0.8))\n",
    "val_dataset = dataset.skip(round(len(dataset) * 0.8))\n",
    "\n",
    "train_dataset = train_dataset.batch(32, drop_remainder=False)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONJV9hgL-hZq"
   },
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.0001))\n",
    "siamese_model.fit(train_dataset, epochs=25, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vqx8fFodxieH"
   },
   "outputs": [],
   "source": [
    "sample = next(iter(train_dataset))\n",
    "anchor, positive, negative = sample\n",
    "anchor_embedding, positive_embedding, negative_embedding = (\n",
    "    embedding(anchor),\n",
    "    embedding(positive),\n",
    "    embedding(negative),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBLY0TOWKTtl"
   },
   "outputs": [],
   "source": [
    "visualize_triplet(sample, y train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5h7IkhJxmw3"
   },
   "outputs": [],
   "source": [
    "#for loop embedding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRK-_8urxTun"
   },
   "outputs": [],
   "source": [
    "cosine_similarity = metrics.CosineSimilarity()\n",
    "\n",
    "positive_similarity = cosine_similarity(anchor_embedding, positive_embedding)\n",
    "print(\"Positive similarity:\", positive_similarity.numpy())\n",
    "\n",
    "negative_similarity = cosine_similarity(anchor_embedding, negative_embedding)\n",
    "print(\"Negative similarity\", negative_similarity.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5wyKJ0LCMFD"
   },
   "source": [
    "Would normalising all features between 0 and 1 help with accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lij1_ryztcc"
   },
   "source": [
    "knn before and after\n",
    "refine network epochs, eval stat, crossfold\n",
    "PCA graph\n",
    "django,"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
